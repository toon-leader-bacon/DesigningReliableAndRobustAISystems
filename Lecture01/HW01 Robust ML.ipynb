{"cells":[{"cell_type":"markdown","metadata":{"id":"dDzfiLFORA7v"},"source":["# Welcome to Homework #1: Robust ML!\n","In this homework assignment you will complete the following three tasks\n","1. Implement Empirical Risk Minimization\n","2. Implement GroupDRO Algorithm following the lecture. For more information you can look at the original paper:\n"," > [Sagawa, Shiori, et al. \"Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization.\" International Conference on Learning Representations (ICLR), 2020.](https://arxiv.org/abs/1911.08731)\n","3. Fine-tune a pre-trained vision transformer\n","4. Compare robustness properties of the three approaches\n","\n","\n","##IMPORTANT:\n","  > Before you get started, select `Runtime > Change runtime type` and select `GPU` for your hardware accelerator.\n","\n","\n","##A couple of notes\n","1. Make sure to run each cell in order!\n","2. Only fill in code in sections marked as follows:\n","```\n","# <<<<<< Put your code here\n","# Write your code\n","# >>>>>\n","```\n","\n","Let's get started!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kIfb_GoAlvGr"},"source":["## Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"t7VeWYBDTCWZ"},"source":["First we have to install the `wilds` python package which will give us access to the Waterbirds dataset. Run the command and give it ~1 min to install."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12840,"status":"ok","timestamp":1690223615849,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"ht-aSxaeO-VD"},"outputs":[],"source":["%%capture\n","!pip install wilds"]},{"cell_type":"markdown","metadata":{"id":"IkR-pJgYTS36"},"source":["Now we load each of the packages we need for the worksheet. See the comments for the purpose of each"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5030,"status":"ok","timestamp":1690223623433,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"Kel5_ptAtPE_"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.0.3) or chardet (4.0.0) doesn't match a supported version!\n","  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"]},{"ename":"RuntimeError","evalue":"Detected that PyTorch and torchvision were compiled with different CUDA versions. PyTorch has CUDA Version=11.7 and torchvision has CUDA Version=11.8. Please reinstall the torchvision that matches your PyTorch install.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m \u001b[39m# the nerual network library used for defining models\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m DataLoader \u001b[39m# Used for wrapping and iterating through out data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39m# Package with utilities for computer vision such as pretrained models\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtransforms\u001b[39;00m \u001b[39m# Set of image transforms for processing image inputs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m \u001b[39m# Numpy used for general array manipulation\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/__init__.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodulefinder\u001b[39;00m \u001b[39mimport\u001b[39;00m Module\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m \u001b[39mimport\u001b[39;00m datasets, io, models, ops, transforms, utils\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_OPS\n\u001b[1;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_optical_flow\u001b[39;00m \u001b[39mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stereo_matching\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     CarlaStereo,\n\u001b[1;32m      4\u001b[0m     CREStereo,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     SintelStereo,\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcaltech\u001b[39;00m \u001b[39mimport\u001b[39;00m Caltech101, Caltech256\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/datasets/_optical_flow.py:12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_png_16\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _read_pfm, verify_str_arg\n\u001b[1;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvision\u001b[39;00m \u001b[39mimport\u001b[39;00m VisionDataset\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/io/__init__.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _log_api_usage_once\n\u001b[1;32m      7\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m----> 8\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_load_gpu_decoder\u001b[39;00m \u001b[39mimport\u001b[39;00m _HAS_GPU_VIDEO_DECODER\n\u001b[1;32m      9\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mModuleNotFoundError\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     _HAS_GPU_VIDEO_DECODER \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/io/_load_gpu_decoder.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextension\u001b[39;00m \u001b[39mimport\u001b[39;00m _load_library\n\u001b[1;32m      4\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m     _load_library(\u001b[39m\"\u001b[39m\u001b[39mDecoder\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/extension.py:107\u001b[0m\n\u001b[1;32m    102\u001b[0m             warn(\u001b[39m\"\u001b[39m\u001b[39mLoadLibraryExW is missing in kernel32.dll\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m     torch\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mload_library(lib_path)\n\u001b[0;32m--> 107\u001b[0m _check_cuda_version()\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchvision/extension.py:80\u001b[0m, in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m     t_minor \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(t_version[\u001b[39m1\u001b[39m])\n\u001b[1;32m     79\u001b[0m     \u001b[39mif\u001b[39;00m t_major \u001b[39m!=\u001b[39m tv_major \u001b[39mor\u001b[39;00m t_minor \u001b[39m!=\u001b[39m tv_minor:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     81\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mDetected that PyTorch and torchvision were compiled with different CUDA versions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPyTorch has CUDA Version=\u001b[39m\u001b[39m{\u001b[39;00mt_major\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mt_minor\u001b[39m}\u001b[39;00m\u001b[39m and torchvision has \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCUDA Version=\u001b[39m\u001b[39m{\u001b[39;00mtv_major\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mtv_minor\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPlease reinstall the torchvision that matches your PyTorch install.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     85\u001b[0m         )\n\u001b[1;32m     86\u001b[0m \u001b[39mreturn\u001b[39;00m _version\n","\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA versions. PyTorch has CUDA Version=11.7 and torchvision has CUDA Version=11.8. Please reinstall the torchvision that matches your PyTorch install."]}],"source":["from wilds import get_dataset # Function to download wilds datasets\n","import torch # Pytorch library used for defining and training ML models\n","import torch.nn as nn # the nerual network library used for defining models\n","from torch.utils.data import DataLoader # Used for wrapping and iterating through out data\n","import torchvision # Package with utilities for computer vision such as pretrained models\n","import torchvision.transforms as transforms # Set of image transforms for processing image inputs\n","\n","import numpy as np # Numpy used for general array manipulation\n","import matplotlib.pyplot as plt # Used for showing images\n","import pandas as pd # Used for storing our results\n","\n","from tqdm import tqdm # Progress bar while training\n","import warnings # Give warnings, such as if CUDA is unavailable\n","import multiprocessing # Allows us to check the number of available CPUS"]},{"cell_type":"markdown","metadata":{"id":"oeD2W1OATctD"},"source":["Now we check that GPU-support is available. If not, you may have forgotten to select the GPU hardware accelerator. We also set some useful constants."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1690223633228,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"KzS5nNW0Ta9T"},"outputs":[],"source":["if not torch.cuda.is_available():\n","    warnings.warn(\n","        \"CUDA is not available, please check that you have selected a GPU for hardware acceleration\"\n","    )\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","N_CORES = multiprocessing.cpu_count()"]},{"cell_type":"markdown","metadata":{"id":"YdrshdD5UhHJ"},"source":["## Download and visualize the dataset"]},{"cell_type":"markdown","metadata":{"id":"WYO-PE_N2pH2"},"source":["Next we will download and visualize the waterbirds dataset. The waterbirds dataset is a synthetic binary classification dataset where the goal is to predict if a bird is a \"Landbird\" or a \"Waterbird\". The birds have been placed on artificial backgrounds where roughly 95% of the time landbirds are against a land backdrop and waterbirds are against a water backdrop. The other 5% of the time landbirds are found over water and waterbirds are found on land. Each of the four confirgurations of \"Landbird vs. Waterbird\" and \"over land vs. over water\" makes up a subgroup. Let's begin!"]},{"cell_type":"markdown","metadata":{"id":"5tp_x4JeW9FK"},"source":["Run the command below to download the waterbirds (should take 1-5 minutes to download). If your session stays active you won't need to re-download this each time you run it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70636,"status":"ok","timestamp":1690223708625,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"f-k2gjTGLaxf","outputId":"b104df2f-b831-4dec-bdb3-fdd3dd9c094b"},"outputs":[],"source":["dataset = get_dataset(dataset=\"waterbirds\", download=False)"]},{"cell_type":"markdown","metadata":{"id":"c-zmU48SUzTH"},"source":["Now lets visualize one training example from each subgroup (remember to read the section above explaining what the four subgroups are)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":261},"executionInfo":{"elapsed":2720,"status":"ok","timestamp":1690223713336,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"KS4ccxzWjVTG","outputId":"0f05f448-8018-4490-dcd0-85864a1f5b7b"},"outputs":[],"source":["# Get the number of groups (should be 4)\n","n_groups = dataset._eval_grouper.n_groups\n","\n","# Get a list of all the group ids for each data point in the dataset\n","group_ids = dataset._eval_grouper.metadata_to_group(dataset.metadata_array)\n","\n","# Define the group and class labels\n","group_labels = [\"Landbird on Land\", \"Landbird on Water\", \"Waterbird on Land\", \"Waterbird on Water\"]\n","class_labels = [\"Landbird\", \"Waterbird\"]\n","\n","# Sample one random image from each group\n","np.random.seed(2)\n","sample_images = [dataset[np.random.choice(np.where(group_ids == i)[0])][0] for i in range(n_groups)]\n","\n","# Plot the figures with the group label as the title\n","fig, axs = plt.subplots(1, n_groups, figsize=(3*n_groups, 3))\n","\n","for i in range(n_groups):\n","    axs[i].imshow(sample_images[i], extent=[0,1,0,1])\n","    axs[i].set_title(group_labels[i])\n","    axs[i].axis('off')\n","\n","plt.subplots_adjust(wspace=0.2)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"Y3dpFrp6Waca"},"source":["## Create Train, Val, Test Splits\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JMR6uetLXCuE"},"source":["We can use the built-in utilities from the `wilds` package, but to do so, we need to define the set of transforms which involves resizing, cropping and then converting the image to a tensor and then normalizing it so the inputs are roughly in the range `[-1, 1]`"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1068,"status":"ok","timestamp":1690223730073,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"iis2jVtrQApO"},"outputs":[],"source":["# Define the set of transforms used in the original paper\n","data_transforms = transforms.Compose(\n","    [\n","        transforms.Resize((256, 256)),\n","        transforms.CenterCrop((224,224)),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225],),\n","    ]\n",")\n","\n","# Built in subset utilities from \"wilds\"\n","train_data = dataset.get_subset(\"train\", transform=data_transforms)\n","val_data = dataset.get_subset(\"val\", transform=data_transforms)\n","test_data = dataset.get_subset(\"test\", transform=data_transforms)\n","\n","# Construct some noisy data (This is used in the later half of the homework)\n","noisy_transform = transforms.Compose([data_transforms, lambda tensor: tensor + torch.randn(tensor.size())])\n","noisy_val_data = dataset.get_subset(\"val\", transform=noisy_transform)"]},{"cell_type":"markdown","metadata":{"id":"A5tUuP5jYoGE"},"source":["Lets find out how many examples of each group are in our training sets"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"executionInfo":{"elapsed":1050,"status":"ok","timestamp":1690223736400,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"FW9n4jX9QQ5s","outputId":"622bc035-1cdc-465f-db96-8cb1b99ebc80"},"outputs":[],"source":["# Get the group id for each of the examples in each split\n","train_groups = dataset._eval_grouper.metadata_to_group(train_data.metadata_array)\n","val_groups = dataset._eval_grouper.metadata_to_group(val_data.metadata_array)\n","test_groups = dataset._eval_grouper.metadata_to_group(test_data.metadata_array)\n","\n","# Count the number of examples of each group type\n","train_examples_per_group = [(train_groups == i).sum().item() for i in range(n_groups)]\n","val_examples_per_group = [(val_groups == i).sum().item() for i in range(n_groups)]\n","test_examples_per_group = [(test_groups == i).sum().item() for i in range(n_groups)]\n","\n","# Store the results in a data frame to display\n","data_summary = pd.DataFrame(columns=[*group_labels, \"Total\"])\n","data_summary.loc[0] = [*train_examples_per_group, sum(train_examples_per_group)]\n","data_summary.loc[1] = [*val_examples_per_group, sum(val_examples_per_group)]\n","data_summary.loc[2] = [*test_examples_per_group, sum(test_examples_per_group)]\n","data_summary.index = ['Training Examples', 'Val Examples', 'Test Examples']\n","data_summary"]},{"cell_type":"markdown","metadata":{"id":"ea47AOMaZmQP"},"source":["## Define our Model and the functions to evaluate it"]},{"cell_type":"markdown","metadata":{"id":"GlhqhIkcaFf6"},"source":["This function loads a `resnet50` model from torchivsion and replaces the final classification layer with a new layer of the appropriate size. The waterbirds classification class is binary (\"Landbird\" or \"waterbird\") so we use two outputs."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":394,"status":"ok","timestamp":1690223739738,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"6MRhP4lUUI8t"},"outputs":[],"source":["def Resnet50(outputs=2):\n","    # Load a resent50 model with the default pre-trained weights\n","    model = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n","\n","    # Replace the Feature Classifier (fc) with a new linear layer with 2 output dimensions\n","    d = model.fc.in_features\n","    model.fc = nn.Linear(d, outputs)\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"BQuFhUf0ak2f"},"source":["This function computes the predictions of the model on the dataset. It returns the true labels, the predicted labels and the group id of each datapoint in the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":333,"status":"ok","timestamp":1690223744565,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"Ncb1BlAcaQ3W"},"outputs":[],"source":["def get_predictions(model, dataset, batch_size=32):\n","    # Put the model on the target device\n","    model = model.to(DEVICE)\n","\n","    # Construct a dataloader with the specified batch size\n","    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=N_CORES)\n","\n","    # Empty arrays for storing labels and predictions and groups\n","    ypred = []\n","    y = []\n","    group = []\n","\n","    # Put the model in eval model and don't store gradients\n","    model.eval()\n","    with torch.no_grad():\n","        # Loop through all data in batches\n","        for inputs, labels, metadata in dataloader:\n","            groups = dataset.dataset._eval_grouper.metadata_to_group(metadata)\n","            inputs = inputs.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            # Compute model outputs and  predictions\n","            outputs = model(inputs)\n","            predicted = torch.argmax(outputs.data, 1)\n","\n","            # Store the results\n","            ypred.extend(predicted.cpu().numpy())\n","            y.extend(labels.cpu().numpy())\n","            group.extend(groups)\n","\n","    return np.array(y), np.array(ypred), np.array(group)"]},{"cell_type":"markdown","metadata":{"id":"7opDFkapfom9"},"source":["This function uses the previous function and computes the per-group performance of the model on the provided dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":496,"status":"ok","timestamp":1690223748239,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"CLc3GnLyfoSm"},"outputs":[],"source":["def evaluate(model, dataset, batch_size=32):\n","    # Get the predictions\n","    y, ypred, group = get_predictions(model, dataset, batch_size)\n","\n","    # Compute accuracy for each group\n","    accuracies = []\n","    for i in range(n_groups):\n","        group_indices = group == i\n","        group_y = y[group_indices]\n","        group_ypred = ypred[group_indices]\n","        accuracy = (group_y == group_ypred).mean()\n","        accuracies.append(accuracy)\n","\n","    # Add the aggregate accuracy\n","    accuracies.append((y == ypred).mean())\n","    return accuracies\n"]},{"cell_type":"markdown","metadata":{"id":"6I-CfqhGf8Id"},"source":["## Empirical Risk Minimization (ERM)\n","\n","---\n","This is where we will begin our implementation, starting with the basic formulation of statistical machine learning: Empirical Risk Minimization.\n"]},{"cell_type":"markdown","metadata":{"id":"kQNQBXROgAlm"},"source":["This function trains a model using empirical risk minimization on the provided dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690223890011,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"jSyq9w5WtRIe"},"outputs":[],"source":["def train_ERM(\n","    model,\n","    dataset,\n","    batch_size=128,\n","    num_epochs=1,\n","    learning_rate=1e-3,\n","    weight_decay=1e-4,\n","):\n","    # Move the model to the device\n","    model = model.to(DEVICE)\n","\n","    # <<<<<< Put your code here\n","    # Define the criterion to be used in the loss function [1 line]\n","    # Hint: you can find a list of loss functions here: https://pytorch.org/docs/stable/nn.html#loss-functions.\n","    \n","    # Quadratic, Cross-Entropy, Weighted Cross-Entropy, etc.\n","    # I'll use CrossEntropy because it's a common loss function.\n","    loss_criterion_func = nn.CrossEntropyLoss()\n","\n","    # >>>>>\n","\n","    # Define the optimizer\n","    optimizer = torch.optim.SGD(\n","        model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay\n","    )\n","\n","    # Construct the DataLoader\n","    dataloader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=True, num_workers=N_CORES\n","    )\n","\n","    # Construct the progess bar\n","    num_batches = len(dataloader)\n","    progress_bar = tqdm(\n","        total=num_batches * num_epochs, desc=\"Training Progress\", position=0, leave=True\n","    )\n","\n","\n","    # Put the model into train mode\n","    model.train()\n","\n","    # Loop over epochs\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        # Loop over data\n","        for (i, (inputs, labels, _)) in enumerate(dataloader):\n","            inputs = inputs.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","\n","            # <<<<<< Put your code here\n","            # Zero the gradients in the optimizezr [1 line]\n","            optimizer.zero_grad()\n","\n","            # Compute the model outputs [1 line]\n","            outputs = model(inputs)\n","\n","            # compute the loss according the criterion [1 line)\n","            # NOTE: Please keep this variable named \"loss\" as it is used later on to keep track of the running loss\n","            # NOTE: criterion is cross entropy loss function\n","            loss = loss_criterion_func(outputs, labels)\n","\n","            # Compute gradients of the loss and step the optimizer [2 lines]\n","            loss.backward() \n","            optimizer.step() # https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html\n","            # >>>>>\n","\n","            # Accumulate the running loss and update the progress bar\n","            running_loss += loss.item()\n","            progress_bar.update(1)\n","            progress_bar.set_postfix({'Epoch': epoch+1, 'Batch': i+1, 'Train_Loss': running_loss / (i+1)})\n","\n","    progress_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"ph1sNTRYjWFr"},"source":["Let's train our first model!\n","This should take about 1 min"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60306,"status":"ok","timestamp":1690223954404,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"uzgACrpNvxAA","outputId":"cbd3f525-1a93-4da7-9b45-4540f04cdc01"},"outputs":[],"source":["ERM_resnet = Resnet50()\n","train_ERM(ERM_resnet, train_data)"]},{"cell_type":"markdown","metadata":{"id":"ZnSvy5rnjc61"},"source":["Now that we have trained our model, let's evaluate it on the validation set to see how well we did"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17271,"status":"ok","timestamp":1690223985582,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"ph6PdUuF16-7"},"outputs":[],"source":["ERM_resnet_val_accuracies = evaluate(ERM_resnet, val_data)\n","ERM_resnet_noisy_val_accuracies = evaluate(ERM_resnet, noisy_val_data) # For later"]},{"cell_type":"markdown","metadata":{"id":"VjzcvcLepl_s"},"source":["Store the results in a table that we will re-use"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690223987309,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"m9QnTUOY6uwd","outputId":"d448650d-9675-4173-e8dd-cbcdd56be73d"},"outputs":[],"source":["results = pd.DataFrame(columns=[*group_labels, \"Aggregate\"])\n","results.loc[0] = ERM_resnet_val_accuracies\n","results.rename(index={0: 'ERM - ResNet - Val'}, inplace=True)\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":314,"status":"ok","timestamp":1690223999954,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"JK2bvcifHu-j"},"outputs":[],"source":["torch.save(ERM_resnet, \"ERM_resnet.ckpt\")\n"]},{"cell_type":"markdown","metadata":{"id":"wq5u8BphuJsS"},"source":["Clear the GPU memory so we can train a new model next"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"elapsed":1017,"status":"error","timestamp":1690225037042,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"1BueqzJmt2cx","outputId":"f75d3000-c3fc-4c53-8815-807ba87ca1c4"},"outputs":[],"source":["del ERM_resnet\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"Sbilm3sGlWSo"},"source":["## Group Distributionally Robust Optimization (GroupDRO)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":384,"status":"ok","timestamp":1690225048976,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"ZVGA_4MuNwzS"},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1690225052399,"user":{"displayName":"xacer c","userId":"02453598874398586068"},"user_tz":240},"id":"5tUV5laG7FD-"},"outputs":[],"source":["def train_GroupDRO(\n","    model,\n","    dataset,\n","    batch_size=128,\n","    num_epochs=1,\n","    learning_rate=1e-3,\n","    weight_decay=1e-4,\n","    step_size=0.01 # This is \\eta_q from the slides\n","):\n","    # Move the model to the device\n","    model = model.to(DEVICE)\n","\n","    # <<<<<< Put your code here\n","    # Define the criterion to be used in the loss function [1 line]\n","    # Using the same cross entropy loss function as the one chosen above\n","    loss_criterion_func = nn.CrossEntropyLoss()\n","    # >>>>>\n","\n","    # Define the optimizer\n","    optimizer = torch.optim.SGD(\n","        model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay\n","    )\n","\n","    # Construct the DataLoader\n","    dataloader = DataLoader(\n","        dataset, batch_size=batch_size, shuffle=True, num_workers=N_CORES\n","    )\n","\n","    # Construct the progress bar\n","    num_batches = len(dataloader)\n","    progress_bar = tqdm(\n","        total=num_batches * num_epochs, desc=\"Training Progress\", position=0, leave=True\n","    )\n","\n","\n","    # Put the model into train mode\n","    model.train()\n","\n","    # <<<<<< Put your code here\n","    # Initialize group weights to all ones [1 line]\n","    # Hint: to get the number of groups use the variable \"n_groups\" which we defined when we loaded in the dataset.\n","    \n","    # Create a tensor of dimension n_groups x 1 long, each element is value 1\n","    group_weights = torch.ones(n_groups)\n","    # >>>>>\n","\n","    # Loop over epochs\n","    for epoch in range(num_epochs):\n","        running_loss = 0.0\n","\n","        # Loop over data\n","        for (i, (inputs, labels, metadata)) in enumerate(dataloader):\n","            inputs = inputs.to(DEVICE)\n","            labels = labels.to(DEVICE)\n","            groups = dataset.dataset._eval_grouper.metadata_to_group(metadata) # (batch_size,)\n","\n","            # <<<<<< Put your code here\n","            # Zero the gradients in the optimizer [1 line]\n","            optimizer.zero_grad()\n","            \n","            # Compute the model outputs [1 line]\n","            # NOTE: Outputs has 2 output dimensions\n","            outputs = model(inputs)\n","            \n","            # Calculate per-group losses [~5 lines]\n","            # Hint: Iterate over the group ids and compute the loss for all inputs that match the group. The \"groups\" variable defined above may be helpful.\n","            losses_by_group = torch.zeros(n_groups)\n","            for output_element_index, group_label in enumerate(groups):\n","                # NOTE: group_label is a number in range [0, 3]\n","                output_at_index = outputs[output_element_index][None]\n","                label_at_index = outputs[output_element_index][None]\n","                losses_by_group[group_label] = loss_criterion_func(output_at_index, label_at_index)\n","\n","            # Update the group weights and normalize them [2 lines]\n","            # Hint: The variable step_size provided as input to the function is the same as \\eta_q in the slides. Use it here.\n","            # NOTE: group_weights are initialized to tensor of all 1s\n","            group_weights = group_weights * (losses_by_group.exp(step_size))\n","            group_weights = group_weights / group_weights.sum()\n","            # group_weights = group_weights * group_losses.exp() * step_size\n","            # group_weights = group_weights / group_weights.sum()\n","\n","            # Compute the weighted average of per-group losses [1 line]\n","            # NOTE: Please keep this variable named \"weighted_loss\" as it is used later on to keep track of the running loss\n","            weighted_loss = torch.sum(group_weights * losses_by_group)\n","\n","            # Compute gradients of the loss and step the optimizer [2 lines]\n","            weighted_loss.backward()\n","            optimizer.step()\n","            # >>>>>\n","\n","            # Accumulate the running loss and update the progress bar\n","            running_loss += weighted_loss.item()\n","            progress_bar.update(1)\n","            progress_bar.set_postfix({'Epoch': epoch+1, 'Batch': i+1, 'Train_Loss': running_loss / (i+1)})\n","\n","    progress_bar.close()"]},{"cell_type":"markdown","metadata":{"id":"obe1CWeXyMBg"},"source":["Train the GroupDRO ResNet50 Model (~1 min)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pf-q8PuVmUBs","outputId":"54c841e0-3bfa-4433-907c-2dd6dfdb2739"},"outputs":[],"source":["GroupDRO_resnet = Resnet50()\n","DEVICE=torch.device(\"cpu\")\n","train_GroupDRO(GroupDRO_resnet, train_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjIxMijrELIK"},"outputs":[],"source":["from google.colab import files\n","torch.save(GroupDRO_resnet, \"GroupDRO_resnet.ckpt\")\n","files.download( \"GroupDRO_resnet.ckpt\" )"]},{"cell_type":"markdown","metadata":{"id":"4QH5klUxyG_a"},"source":["Evaluate the GroupDRO model on the validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulGpBlmVph_t"},"outputs":[],"source":["GroupDRO_resnet_val_accuracies = evaluate(GroupDRO_resnet, val_data)\n","GroupDRO_resnet_noisy_val_accuracies = evaluate(GroupDRO_resnet, noisy_val_data) # For later"]},{"cell_type":"markdown","metadata":{"id":"04i1SLJjyD0d"},"source":["Store the results in our table"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VbegWwsnw3_Z"},"outputs":[],"source":["results.loc[1] = GroupDRO_resnet_val_accuracies\n","results.rename(index={1: 'GroupDRO - ResNet - Val'}, inplace=True)\n","results"]},{"cell_type":"markdown","metadata":{"id":"KGqAwxM30veO"},"source":["How does GroupDRO compare to ERM?"]},{"cell_type":"markdown","metadata":{"id":"Ky9M6aYUx8kc"},"source":["Clear the GPU memory so we can train a new model next"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dH6A9QhwwuDx"},"outputs":[],"source":["del GroupDRO_resnet\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"MPQOWe5jyS8t"},"source":["## Fine-Tuning a Vision Transformer"]},{"cell_type":"markdown","metadata":{"id":"ZK1b8Rz1zFTa"},"source":["This function will return a Vision Transformer that is ready to be fine-tuned. It should be intitialized with the default set of weights.\n","\n","To enable fine-tuning we will freeze all but the final layer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ChTFQbrryV_A"},"outputs":[],"source":["def ViT(output=2):\n","    # <<<<<< Put your code here\n","    # Load vit_b_16 torchvision model with weights=torchvision.models.ViT_B_16_Weights.DEFAULT\n","    model = torchvision.models.vit_b_16(weights=torchvision.models.ViT_B_16_Weights.DEFAULT)\n","\n","    # Disable all of the gradients for all parameters [2 lines]\n","    for param in model.features.parameters():\n","        param.requires_grad = False\n","\n","    # Replace the last \"head\" with a new linear layer of the appropriate size [~2 lines]\n","    # HINT: The last layer is in list called \"heads\".\n","    # HINT: Refer to the ResNet50() function above to see how to make a new layer\n","    d = model.fc.in_features\n","    model.fc = nn.Linear(d, output)\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"JzvxxHpyzXXn"},"source":["Train a ViT using ERM and evaluate (~2-3 min)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pi5hNdqFzWzf"},"outputs":[],"source":["ERM_vit = ViT()\n","train_ERM(ERM_vit, train_data)\n","ERM_vit_val_accuracies = evaluate(ERM_vit, val_data)\n","ERM_vit_noisy_val_accuracies = evaluate(ERM_vit, noisy_val_data)\n","del ERM_vit\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"KgzqsjsLza9o"},"source":["Train a ViT using GroupDRO and evaluate  (~2-3 min)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c57z-0rA0JYU"},"outputs":[],"source":["GroupDRO_vit = ViT()\n","train_GroupDRO(GroupDRO_vit, train_data)\n","GroupDRO_vit_val_accuracies = evaluate(GroupDRO_vit, val_data)\n","GroupDRO_vit_noisy_val_accuracies = evaluate(GroupDRO_vit, noisy_val_data)\n","del GroupDRO_vit\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"60Y_gPC_0PWb"},"outputs":[],"source":["results.loc[2] = ERM_vit_val_accuracies\n","results.loc[3] = GroupDRO_vit_val_accuracies\n","results.rename(index={2: 'ERM - ViT - Val'}, inplace=True)\n","results.rename(index={3: 'GroupDRO - ViT - Val'}, inplace=True)\n","results"]},{"cell_type":"markdown","metadata":{"id":"DD4R3UYz1j0A"},"source":["## Robustness to Noise"]},{"cell_type":"markdown","metadata":{"id":"tqwAFAM61sfw"},"source":["In addition to evaluating on the validation set, we created a \"noisy\" validation set that included Gaussian noise on the inputs.\n","\n","An example of Gaussian noise is below\n","![](https://www.researchgate.net/publication/221913964/figure/fig7/AS:305170975084549@1449769838319/Examples-of-images-modified-by-Gaussian-noise-Gaussian-noise-was-applied-on-each-image.png)\n","\n","In general, a model's performance will drop due to this perturbation, but a robust model will retain its performance better. Let's see how the different model architecture compare"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nngm3gDV1Sdd"},"outputs":[],"source":["noisy_results = pd.DataFrame(columns=[\"Clean Accuracy\", \"Noisy Accuracy\"])\n","noisy_results.loc[0] = [GroupDRO_resnet_val_accuracies[-1], GroupDRO_resnet_noisy_val_accuracies[-1]]\n","noisy_results.loc[1] = [GroupDRO_vit_val_accuracies[-1], GroupDRO_vit_noisy_val_accuracies[-1]]\n","noisy_results.rename(index={0: 'DRO - ResNet'}, inplace=True)\n","noisy_results.rename(index={1: 'DRO - ViT'}, inplace=True)\n","noisy_results"]},{"cell_type":"markdown","metadata":{"id":"MoTPjfjz6BjT"},"source":["Does the pre-trained ViT improve robustness to noise?"]},{"cell_type":"markdown","metadata":{"id":"PWwWymUs5_Dj"},"source":["## Conclusion and Bonus Activities\n","\n","\n","---\n","\n","\n","\n","Congratulations on completing the assignment! You can stop here, or if you want to try out some additional concepts from lecture consider trying one of the following\n","\n","\n","1. Experiment with other architectures and pre-training types. You can find a list of pre-trained models [here](https://pytorch.org/vision/stable/models.html)\n","2. Experiment with robustness to other types of transformations, or try augmentating your training data with them. Available transforms in torchvision are [here](https://pytorch.org/vision/main/transforms.html)\n","3. Implement DeepCORAL [paper](https://arxiv.org/abs/1607.01719), [sample code](https://github.com/DenisDsh/PyTorch-Deep-CORAL)   \n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
